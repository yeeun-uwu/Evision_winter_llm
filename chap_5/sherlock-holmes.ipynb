{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf19f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapter import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # 추론 시에는 드롭아웃을 비활성화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50305461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "출력 텍스트: Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from previous_chapter import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special = {'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(f\"출력 텍스트: {token_ids_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "780db428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자: 560713\n",
      "토큰: 145658\n"
     ]
    }
   ],
   "source": [
    "file_path = \"The-adventures-of-sherlock-holmes.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"문자:\", total_characters)\n",
    "print(\"토큰:\", total_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7614a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 로더:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "검증 데이터 로더:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "from previous_chapter import create_dataloader_v1\n",
    "\n",
    "# 훈련 세트 비율\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"훈련 데이터 로더:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\n검증 데이터 로더:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc0a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212cf85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n",
      "훈련 손실: 10.961420860141516\n",
      "검증 손실: 10.977887289864677\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "    if (major, minor) >= (2, 9):\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) \n",
    "\n",
    "torch.manual_seed(123) \n",
    "\n",
    "with torch.no_grad(): \n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"훈련 손실:\", train_loss)\n",
    "print(\"검증 손실:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f73886e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  \n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() \n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"에포크 {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"훈련 손실 {train_loss:.3f}, 검증 손실 {val_loss:.3f}\")\n",
    "\n",
    "        # 샘플 텍스트 출력\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # 간결한 출력 포맷\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6db19570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 1 (Step 000000): 훈련 손실 9.660, 검증 손실 9.787\n",
      "에포크 1 (Step 000005): 훈련 손실 7.898, 검증 손실 8.135\n",
      "에포크 1 (Step 000010): 훈련 손실 6.660, 검증 손실 7.152\n",
      "에포크 1 (Step 000015): 훈련 손실 6.428, 검증 손실 6.831\n",
      "에포크 1 (Step 000020): 훈련 손실 6.504, 검증 손실 6.735\n",
      "에포크 1 (Step 000025): 훈련 손실 6.330, 검증 손실 6.618\n",
      "에포크 1 (Step 000030): 훈련 손실 5.909, 검증 손실 6.510\n",
      "에포크 1 (Step 000035): 훈련 손실 6.043, 검증 손실 6.456\n",
      "에포크 1 (Step 000040): 훈련 손실 5.906, 검증 손실 6.403\n",
      "에포크 1 (Step 000045): 훈련 손실 5.958, 검증 손실 6.318\n",
      "에포크 1 (Step 000050): 훈련 손실 6.140, 검증 손실 6.262\n",
      "에포크 1 (Step 000055): 훈련 손실 5.618, 검증 손실 6.257\n",
      "에포크 1 (Step 000060): 훈련 손실 5.902, 검증 손실 6.188\n",
      "에포크 1 (Step 000065): 훈련 손실 5.761, 검증 손실 6.125\n",
      "에포크 1 (Step 000070): 훈련 손실 5.500, 검증 손실 6.033\n",
      "에포크 1 (Step 000075): 훈련 손실 6.010, 검증 손실 6.042\n",
      "에포크 1 (Step 000080): 훈련 손실 5.705, 검증 손실 5.997\n",
      "에포크 1 (Step 000085): 훈련 손실 5.339, 검증 손실 6.001\n",
      "에포크 1 (Step 000090): 훈련 손실 5.411, 검증 손실 5.948\n",
      "에포크 1 (Step 000095): 훈련 손실 5.395, 검증 손실 5.931\n",
      "에포크 1 (Step 000100): 훈련 손실 5.572, 검증 손실 5.902\n",
      "에포크 1 (Step 000105): 훈련 손실 5.421, 검증 손실 5.892\n",
      "에포크 1 (Step 000110): 훈련 손실 5.466, 검증 손실 5.879\n",
      "에포크 1 (Step 000115): 훈련 손실 5.251, 검증 손실 5.848\n",
      "에포크 1 (Step 000120): 훈련 손실 5.360, 검증 손실 5.839\n",
      "에포크 1 (Step 000125): 훈련 손실 5.335, 검증 손실 5.816\n",
      "에포크 1 (Step 000130): 훈련 손실 5.379, 검증 손실 5.782\n",
      "에포크 1 (Step 000135): 훈련 손실 5.439, 검증 손실 5.758\n",
      "에포크 1 (Step 000140): 훈련 손실 5.417, 검증 손실 5.751\n",
      "에포크 1 (Step 000145): 훈련 손실 5.612, 검증 손실 5.731\n",
      "에포크 1 (Step 000150): 훈련 손실 5.283, 검증 손실 5.704\n",
      "에포크 1 (Step 000155): 훈련 손실 4.787, 검증 손실 5.715\n",
      "에포크 1 (Step 000160): 훈련 손실 5.643, 검증 손실 5.684\n",
      "에포크 1 (Step 000165): 훈련 손실 5.325, 검증 손실 5.665\n",
      "에포크 1 (Step 000170): 훈련 손실 5.109, 검증 손실 5.698\n",
      "에포크 1 (Step 000175): 훈련 손실 4.921, 검증 손실 5.659\n",
      "에포크 1 (Step 000180): 훈련 손실 5.212, 검증 손실 5.655\n",
      "에포크 1 (Step 000185): 훈련 손실 5.018, 검증 손실 5.635\n",
      "에포크 1 (Step 000190): 훈련 손실 4.738, 검증 손실 5.617\n",
      "에포크 1 (Step 000195): 훈련 손실 4.912, 검증 손실 5.624\n",
      "에포크 1 (Step 000200): 훈련 손실 4.990, 검증 손실 5.632\n",
      "에포크 1 (Step 000205): 훈련 손실 5.038, 검증 손실 5.621\n",
      "에포크 1 (Step 000210): 훈련 손실 5.053, 검증 손실 5.607\n",
      "에포크 1 (Step 000215): 훈련 손실 5.112, 검증 손실 5.601\n",
      "에포크 1 (Step 000220): 훈련 손실 5.301, 검증 손실 5.580\n",
      "에포크 1 (Step 000225): 훈련 손실 5.202, 검증 손실 5.595\n",
      "에포크 1 (Step 000230): 훈련 손실 4.906, 검증 손실 5.562\n",
      "에포크 1 (Step 000235): 훈련 손실 4.913, 검증 손실 5.563\n",
      "에포크 1 (Step 000240): 훈련 손실 4.874, 검증 손실 5.547\n",
      "에포크 1 (Step 000245): 훈련 손실 5.095, 검증 손실 5.549\n",
      "에포크 1 (Step 000250): 훈련 손실 4.920, 검증 손실 5.547\n",
      "에포크 1 (Step 000255): 훈련 손실 5.206, 검증 손실 5.515\n",
      "Every effort moves you.”   “I am not not not not not not not not not not be a very very very very very very very very very very very very very very man,“I am not not not not not a very very\n",
      "에포크 2 (Step 000260): 훈련 손실 4.915, 검증 손실 5.549\n",
      "에포크 2 (Step 000265): 훈련 손실 5.356, 검증 손실 5.558\n",
      "에포크 2 (Step 000270): 훈련 손실 4.650, 검증 손실 5.544\n",
      "에포크 2 (Step 000275): 훈련 손실 4.983, 검증 손실 5.530\n",
      "에포크 2 (Step 000280): 훈련 손실 5.149, 검증 손실 5.564\n",
      "에포크 2 (Step 000285): 훈련 손실 5.128, 검증 손실 5.538\n",
      "에포크 2 (Step 000290): 훈련 손실 4.988, 검증 손실 5.569\n",
      "에포크 2 (Step 000295): 훈련 손실 4.981, 검증 손실 5.548\n",
      "에포크 2 (Step 000300): 훈련 손실 4.908, 검증 손실 5.560\n",
      "에포크 2 (Step 000305): 훈련 손실 4.831, 검증 손실 5.524\n",
      "에포크 2 (Step 000310): 훈련 손실 4.797, 검증 손실 5.567\n",
      "에포크 2 (Step 000315): 훈련 손실 4.792, 검증 손실 5.541\n",
      "에포크 2 (Step 000320): 훈련 손실 4.984, 검증 손실 5.527\n",
      "에포크 2 (Step 000325): 훈련 손실 4.979, 검증 손실 5.538\n",
      "에포크 2 (Step 000330): 훈련 손실 4.333, 검증 손실 5.534\n",
      "에포크 2 (Step 000335): 훈련 손실 4.651, 검증 손실 5.525\n",
      "에포크 2 (Step 000340): 훈련 손실 4.518, 검증 손실 5.513\n",
      "에포크 2 (Step 000345): 훈련 손실 4.835, 검증 손실 5.496\n",
      "에포크 2 (Step 000350): 훈련 손실 4.654, 검증 손실 5.497\n",
      "에포크 2 (Step 000355): 훈련 손실 5.097, 검증 손실 5.522\n",
      "에포크 2 (Step 000360): 훈련 손실 4.681, 검증 손실 5.496\n",
      "에포크 2 (Step 000365): 훈련 손실 4.972, 검증 손실 5.478\n",
      "에포크 2 (Step 000370): 훈련 손실 4.947, 검증 손실 5.454\n",
      "에포크 2 (Step 000375): 훈련 손실 4.637, 검증 손실 5.471\n",
      "에포크 2 (Step 000380): 훈련 손실 4.590, 검증 손실 5.461\n",
      "에포크 2 (Step 000385): 훈련 손실 4.747, 검증 손실 5.446\n",
      "에포크 2 (Step 000390): 훈련 손실 4.651, 검증 손실 5.444\n",
      "에포크 2 (Step 000395): 훈련 손실 4.737, 검증 손실 5.480\n",
      "에포크 2 (Step 000400): 훈련 손실 4.762, 검증 손실 5.455\n",
      "에포크 2 (Step 000405): 훈련 손실 4.718, 검증 손실 5.465\n",
      "에포크 2 (Step 000410): 훈련 손실 4.743, 검증 손실 5.444\n",
      "에포크 2 (Step 000415): 훈련 손실 4.681, 검증 손실 5.502\n",
      "에포크 2 (Step 000420): 훈련 손실 4.502, 검증 손실 5.468\n",
      "에포크 2 (Step 000425): 훈련 손실 4.876, 검증 손실 5.481\n",
      "에포크 2 (Step 000430): 훈련 손실 4.477, 검증 손실 5.447\n",
      "에포크 2 (Step 000435): 훈련 손실 4.717, 검증 손실 5.423\n",
      "에포크 2 (Step 000440): 훈련 손실 4.858, 검증 손실 5.451\n",
      "에포크 2 (Step 000445): 훈련 손실 4.814, 검증 손실 5.448\n",
      "에포크 2 (Step 000450): 훈련 손실 4.521, 검증 손실 5.450\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m optimizer = torch.optim.AdamW(model.parameters(), lr=\u001b[32m0.0004\u001b[39m, weight_decay=\u001b[32m0.1\u001b[39m)\n\u001b[32m      9\u001b[39m num_epochs = \u001b[32m7\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train_losses, val_losses, tokens_seen = \u001b[43mtrain_model_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvery effort moves you\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m end_time = time.time()\n\u001b[32m     17\u001b[39m execution_time_minutes = (end_time - start_time) / \u001b[32m60\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtrain_model_simple\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m input_batch, target_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     10\u001b[39m     optimizer.zero_grad() \n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     loss = \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     loss.backward()\n\u001b[32m     13\u001b[39m     optimizer.step() \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mcalc_loss_batch\u001b[39m\u001b[34m(input_batch, target_batch, model, device)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalc_loss_batch\u001b[39m(input_batch, target_batch, model, device):\n\u001b[32m      2\u001b[39m     input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     loss = torch.nn.functional.cross_entropy(logits.flatten(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m), target_batch.flatten())\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saki0\\Documents\\GitHub\\Evision_winter_llm\\chap_5\\previous_chapter.py:209\u001b[39m, in \u001b[36mGPTModel.forward\u001b[39m\u001b[34m(self, in_idx)\u001b[39m\n\u001b[32m    207\u001b[39m x = tok_embeds + pos_embeds  \u001b[38;5;66;03m# Shape [batch_size, num_tokens, emb_size]\u001b[39;00m\n\u001b[32m    208\u001b[39m x = \u001b[38;5;28mself\u001b[39m.drop_emb(x)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrf_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m x = \u001b[38;5;28mself\u001b[39m.final_norm(x)\n\u001b[32m    211\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.out_head(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saki0\\Documents\\GitHub\\Evision_winter_llm\\chap_5\\previous_chapter.py:176\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    174\u001b[39m shortcut = x\n\u001b[32m    175\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm1(x)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43matt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# Shape [batch_size, num_tokens, emb_size]\u001b[39;00m\n\u001b[32m    177\u001b[39m x = \u001b[38;5;28mself\u001b[39m.drop_shortcut(x)\n\u001b[32m    178\u001b[39m x = x + shortcut  \u001b[38;5;66;03m# Add the original input back\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saki0\\Documents\\GitHub\\Evision_winter_llm\\chap_5\\previous_chapter.py:104\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    101\u001b[39m attn_scores.masked_fill_(mask_bool, -torch.inf)\n\u001b[32m    103\u001b[39m attn_weights = torch.softmax(attn_scores / keys.shape[-\u001b[32m1\u001b[39m]**\u001b[32m0.5\u001b[39m, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Shape: (b, num_tokens, num_heads, head_dim)\u001b[39;00m\n\u001b[32m    107\u001b[39m context_vec = (attn_weights @ values).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\dropout.py:73\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m     70\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\functional.py:1418\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1416\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1417\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1418\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1419\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 7\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"훈련 소요 시간: {execution_time_minutes:.2f}분.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69acc6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAATn1JREFUeJzt3QV81PX/B/DXuliwYEFt1OgOaRUkpVQwEAkFpRUDUVFQBEFEBBHrD/wMREVKGumu0Z0jx2Cs2NhY3P/x/ty+t9sYsMG2i72ej8eXq+/dffbl7t7fT75tdDqdDkRERGSWbE1dACIiIro3BmoiIiIzxkBNRERkxhioiYiIzBgDNRERkRljoCYiIjJjDNRERERmjIGaiIjIjDFQExERmTEGaiIrcP78edjY2GD//v2mLgoR5TMGaiIzIYH2ftuYMWNMXUQiMgF7U7wpEd3t6tWrhut//vknPv74Y5w4ccJwX7FixUxUMiIyJdaoicxEQECAYfP09FS1aO12iRIlMGXKFJQqVQpOTk6oXbs2Vq5cec/XSktLQ79+/VC5cmVcuHBB3bd48WLUrVsXzs7OKFeuHMaOHYvU1FTDc+T9fv75Z3Tr1g2urq6oWLEilixZYng8OjoaPXv2hJ+fH1xcXNTjs2fPvmcZ5s+fjxo1aqh9fXx80Lp1ayQkJBgel/eqUqWKKo+U87vvvsvy/IsXL6JHjx7w8vKCt7c3unTpopr4NX369EHXrl0xefJkBAYGqvcYPHgwUlJSHuLoE5kxyZ5FROZl9uzZOk9PT8PtKVOm6Dw8PHR//PGH7vjx47r33ntP5+DgoDt58qR6/Ny5c5IFT7dv3z5dUlKSrlu3bro6deroIiMj1eObNm1Sz58zZ47uzJkzutWrV+uCg4N1Y8aMMbyHPL9UqVK6uXPn6k6dOqUbNmyYrlixYrqoqCj1+ODBg3W1a9fW7d69W73fmjVrdEuWLMmx/FeuXNHZ29urcsu+Bw8e1M2YMUMXHx+vHv/tt990gYGBun/++Ud39uxZdent7a3KJ+7cuaOrUqWKrl+/fuq5R48e1b300ku60NBQXXJystqnd+/e6m964403dMeOHdP9+++/OldXV92PP/5YYP8vRKbAQE1kAYE6KChI9/nnn2fZp0GDBrpBgwZlCdSbN2/WtWrVStesWTNdTEyMYV+5b/z48Vme/+uvv6pgqZHnf/TRR4bbt27dUvetWLFC3e7UqZOub9++uSr/3r171XPPnz+f4+Ply5dXJwTGPvvsM13jxo0NZZOgnJ6ebnhcArSLi4tu1apVhkBdtmxZXWpqqmGf7t27655//vlclZHIUrCPmsjMxcXF4cqVK2jatGmW++X2gQMHstz34osvqubxdevWqSZnjey3detWfP7551max5OSkpCYmKiaukXNmjUNj7u5ucHDwwORkZHq9sCBA/Hss88iLCwMbdq0Uc3OTZo0ybHMtWrVQqtWrVTTd9u2bdX+zz33HIoXL66av8+cOYNXX30V/fv3NzxHmuGlyV8r7+nTp+Hu7p7ldaW88lxNtWrVYGdnZ7gtTeCHDh3K9bElsgQM1ERWpEOHDvjtt9+wfft2PPnkk4b7b926pfqkn3nmmbueI33EGgcHhyyPSb91enq6ut6+fXuEh4dj+fLlWLNmjQrE0icsfcTZSfCUfbZt24bVq1dj+vTp+PDDD7Fz507DScFPP/2ERo0a3fU8rbz16tXD77//ftdrSx95bspLZC0YqInMnNRqg4KCVI24ZcuWhvvldsOGDbPsK7Xe6tWro3Pnzli2bJlhfxlEJiPIK1So8EhlkSDZu3dvtTVv3hzvvvtujoFaC5pS65dNRrCXLVsWCxcuxIgRI9Tfc/bsWTU4LSdSXhn5LoPo5O8nKsoYqIksgATETz75BOXLl1cjvmW0tSxuklONc+jQoapZ++mnn8aKFSvQrFkzFSjldpkyZVQTtK2trWpePnz4MMaNG5erMshrSC1XmpuTk5OxdOlSNWo7J1JzXrt2rWrylmArt69fv27YX2r3w4YNU03d7dq1U6+3Z88eNbJcArkE8C+//FKN9P70009Vc77U5hcsWID33ntP3SYqKhioiSyABLXY2Fi8/fbbqs+4atWqauqUTJHKyZtvvqmagKUpXKZxST+xBFYJehMnTlRNxjIl6rXXXst1GRwdHTFq1Cg1RUr6v6VGPW/evBz3lVrwpk2bMHXqVNXHLrXpr776SjWfC3lfaQKXYCwnIdIfLv3ZUm4hj8nzR44cqZrr4+PjUbJkSdXczho2FTU2MqLM1IUgIiKinHHBEyIiIjPGQE1ERGTGGKiJiIjMGAM1ERGRGWOgJiIiMmMM1ERERGaMgfoeZsyYgeDgYLW8oixzuGvXLlMXySzI3NZOnTqplaVk5alFixZleVxm+8nCGLLmssy1ldSGp06dyrLPzZs31YIWMh9WUhjKms+yZKSxgwcPqnm6cvxLly6NSZMm3VWWv//+W80Fln1kDq4sbWnJJkyYgAYNGqj1rWWREFlL2zgftbbWtSzbKSkdJT+1rL197dq1LPtIWsuOHTuqucjyOjJP2TidpdiwYYNa/UtSZspqZXPmzCkS34GZM2eq9czlsydb48aN1aIwGh7f/PXFF1+o3wltfrzgMX4Ips4KYo7mzZunc3R01M2aNUt35MgRXf/+/XVeXl66a9eu6Yq65cuX6z788EPdggULVHakhQsXZnn8iy++UFmfFi1apDtw4ICuc+fOupCQEN3t27cN+7Rr105Xq1Yt3Y4dO1S2pwoVKuhefPFFw+OxsbE6f39/Xc+ePXWHDx9WqR0la9IPP/xg2Gfr1q06Ozs73aRJk1QKRMn6JGkfDx06pLNUbdu2VVmz5G/ev3+/rkOHDroyZcqoLFYaSelYunRp3dq1a3V79uzRPfbYY7omTZoYHpdMUtWrV9e1bt1apbyU/y9fX1/dqFGjDPtIWklJBzlixAh17KZPn66O5cqVK63+OyBpOZctW6bSg544cUL3wQcfqM+NHHPB45t/du3apVKp1qxZUzd8+HDD/TzGecdAnYOGDRuq3LuatLQ0lWZwwoQJJi2XuckeqCUlYUBAgO7LL7803CepFp2cnFSwFfKlkudJTmONpFG0sbHRXb58Wd3+7rvvdMWLFzfkHRYjR45UaQ81PXr00HXs2DFLeRo1aqR7/fXXddZCcknLsdq4caPhWEpQ+fvvvw37SB5m2Wf79u3qtvyo2dra6iIiIgz7zJw5U+Vt1o6n5LKuVq1alveS1JByolAUvwPyWfv55595fPOR5B2vWLGiylnesmVLQ6DmMX44bPrO5s6dO9i7d69qstXIushyWzIS0b2dO3cOERERWY6drOUsTU7asZNLae6uX7++YR/ZX46xrAet7dOiRQu1ZKVGlsCUZmBZC1rbx/h9tH2s6f9IlgwV3t7e6lI+lykpKVn+bmn6l/W7jY+vdAP4+/tnOS6yjOeRI0dydeyKyndA1kOXJVAl7aY0gfP45h9p2pam6+zHgcf44XCt72xu3LihvsDGHxIht48fP26yclkCCdIip2OnPSaX0udkzN7eXgUj431CQkLueg3tMclpLJf3ex9LJ+t0S7+eZJ6SbFhC/jY5eZETnfsd35yOi/bY/faRH8Lbt2+rkyFr/g5IvmoJzNJXKn2kktFL1k6XJCc8vo9OTn4kZ/nu3bvveoyf4YfDQE1kpjUSyWy1ZcsWUxfF6oSGhqqgLC0W8+fPVyk7N27caOpiWYWLFy9i+PDhKhe5cZ5zejRs+s7G19dXJa/PPgpRbgcEBJisXJZAOz73O3ZyKdmfjMloThkJbrxPTq9h/B732sca/o+GDBmiMl2tX78+SzpH+dukSS8mJua+x/dhj52MgpaR+tb+HZAanYwSlpSdMtK+Vq1a+Oabb3h884E0N8v3W0ZjS0uZbHISNG3aNHVdarQ8xnnHQJ3Dl1i+wJJL17gZUm5LcxndmzRXy5fA+NhJU5T0PWvHTi7lSypfaM26devUMZa+bG0fmQYmfVkaOUOXmpA0e2v7GL+Pto8l/x/J+DwJ0tIUK8cke/O/fC4lPaXx3y399jKVxfj4StOu8cmQHBf5AZPm3dwcu6L2HZC/TfJh8/g+OklDKsdHWiy0TcajyHRM7TqP8UN4yEFoVk2G9ctI5Tlz5qhRygMGDFDD+o1HIRZVMppTpkzIJh+fKVOmqOvh4eGG6VlyrBYvXqw7ePCgrkuXLjlOz6pTp45u586dui1btqjRocbTs2RkqEzP6tWrl5o2I/8fMhUj+/Qse3t73eTJk9Wo0U8++cTip2cNHDhQTW3bsGGD7urVq4YtMTExy9QWmbK1bt06NbWlcePGass+taVNmzZqipdMV/Hz88txasu7776rjt2MGTNynNpijd+B999/X42iP3funPp8ym2ZcbB69Wr1OI9v/jMe9S14jPOOgfoeZF6efJhkHp4M85c5v6TTrV+/XgXo7Fvv3r0NU7RGjx6tAq18SVq1aqXmqxqLiopSgblYsWJqykXfvn3VCYAxmYPdrFkz9RolS5ZUJwDZ/fXXX7pKlSqp/yOZqiHzYy1ZTsdVNplbrZETnkGDBqkpRfJD1a1bNxXMjZ0/f17Xvn17Nfdc5p++/fbbupSUlLv+H2vXrq2OXbly5bK8hzV/B/r166crW7as+pvkx18+n1qQFjy+BR+oeYzzzkb+eZiaOBERERU89lETERGZMQZqIiIiM8ZATUREZMYYqImIiMwYAzUREZEZY6AmIiIyYwzU9yGrFY0ZM0ZdUv7j8S1YPL4Fj8e4YPH46nEe9X3I8peSplEW75fl6yh/8fgWLB7fgsdjXLB4fPVYoyYiIjJjDNRERERmzOrzUUsKxX379qn0ara2eTsviY+PV5eXL19WTTCUv3h8CxaPb8HjMS5Y1nx809PTVdrNOnXqqBSg92P1fdS7d+9Gw4YNTV0MIiKiu+zatQsNGjRAka5RS01aOxiBgYGmLg4RERGuXr2qKpFajCrSgVpr7pYgXapUKVMXh4iIyCA3XbImHUy2adMmdOrUCUFBQbCxscGiRYuyPC6t8h9//LEKsi4uLmjdujVOnTplsvISEREVNpMG6oSEBNSqVQszZszI8fFJkyZh2rRp+P7777Fz5064ubmhbdu2SEpKKvSyEhERmYJJm77bt2+vtpxIbXrq1Kn46KOP0KVLF3XfL7/8otrzpeb9wgsvFHJpiYiICp/Z9lGfO3cOERERqrlbIyvUNGrUCNu3b2egJqICkZaWhpSUFFMXgyycg4MD7OzsrDtQS5AW2UfEyW3tsZzImrDG68Jq8/CIiO5HWvHktyUmJsbURSEr4eXlhYCAADUGyyoD9cOaMGECxo4dWzAvnpYKrB0LhLQEKmbW9InI8mlBukSJEnB1dX3kH1cq2id9iYmJiIyMVLcfdWqw2QZqOQsRsnKL8R8pt2vXrn3P540aNQojRoww3JYVbapWrZo/hdr1I7BtGhD2P2DABsC7XP68LhGZvLlbC9I+Pj6mLg5ZARcXF3UpwVo+V4/SDG62a32HhISoYL127VrDfbKEnIz+bty48T2f5+TkpLKsaJu7u3u+lWm+bVucdaoCJMUC83oCybfy7bWJyHS0PmmpSRPlF+3z9KhjHkwaqG/duoX9+/erTRtAJtcvXLigmp3efPNNjBs3DkuWLMGhQ4fwyiuvqDnXXbt2LfSyXom5jQ//PYkXYwcjwcEHiDwKLBkibRyFXhYiKhhs7iZz/DyZNFDv2bNHLUgum5Ama7kui5yI9957D0OHDsWAAQPUWqgS2FeuXAlnZ+dCL2uQlws+61od1+CNPglDkG5jDxxZCGz9ptDLQkRERYdJA/Xjjz+uOt2zb3PmzDGcjXz66adqkIcscvLff/+hUqVKJitvj/ql0aN+KexOD8VEm776O2Vw2enM5nkiIksXHBys1rHIrQ0bNqjf64IeMT9nzhw1krqoMds+anP1aZfqqBzgjh8SH8dal7aALh2Y3w+4ec7URSOiIkaC4/22MWPGPHTWQWnJzK0mTZqoJBOy1gXlPwbqPHJ2sMPMl+uhmJMDBka/hMtu1YCkGODPl4E7CaYuHhEVIRIctU1qwDKA1vi+d955x7CvtFampqbm6nX9/PzyNLDO0dExX+YLU84YqB9CiK8bJj1XE3fggGeiBiLZ2Re4dhhYzMFlRFR4JDhqm9RmJVBqt48fP65mvaxYsQL16tVTM2K2bNmCM2fOqGWZZfGoYsWKqfE/0q14v6Zved2ff/4Z3bp1UwG8YsWKapDvvZq+tSbqVatWoUqVKup92rVrp04eNHLSMGzYMLWfTIkbOXIkevfunefBwjNnzkT58uXVyUJoaCh+/fXXLCcn0qpQpkwZ9ffLYGR5T813332n/hYZ9yTH47nnnoM5YqB+SB1qBKJPk2A1uGxA0jDobGVw2QJg23RTF42I8mvRijupJtnkvfPL+++/jy+++ALHjh1DzZo11aDcDh06qKmv+/btUwFUshjKbJv7kYWkevTogYMHD6rn9+zZEzdv3rzn/rLgx+TJk1XglEyJ8vrGNfyJEyfi999/x+zZs7F161Y1/TZ7BsUHWbhwIYYPH463334bhw8fxuuvv46+ffti/fr16vF//vkHX3/9NX744QeVeVFev0aNGobBzBK0ZRzUiRMn1EDlFi1awByZ7YInluCDDlWw/2IMNl6sgO99+mNgwkxg4ySgdk/AjYsmEFmy2ylpqPrxKpO899FP28LVMX9+niUQPfXUU4bb3t7eKmuh5rPPPlMBT2rIQ4YMuefr9OnTBy+++KK6Pn78eJXZcNeuXSrQ50TmDkvmQ6ntCnltKYtm+vTpaoEqqaWLb7/9FsuXL8/T3zZ58mRVrkGDBhlmDu3YsUPd/8QTT6iTA2ldkJwRsva21KwbNmyo9pXHJCPj008/rVoeypYta5iBZG5Yo34Ejva2mNGzLrxcHTAxqhnW+/cG+q1kkCYis1G/fv0st6VGLTVbaZKWZmdplpba9oNq1FIb10iAk/5wbYnMnEgTuRakhawwqe0fGxurVpnUgqaQlbukiT4vjh07hqZNm2a5T27L/aJ79+64ffs2ypUrh/79+6sTEq2fXk5eJDjLY7169VK1e2kFMEesUT+ikl4u+Pr52ug7ezf6hrfFNxHF0UW/+ikRWTAXBztVszXVe+cXCarGJEivWbNG1TorVKiglrqUvtk7d+7c93WkRmpM+qTT09PztH9+NunnRunSpVWztvTBy98sNe8vv/wSGzduVLXosLAw1b++evVqtX6H9GfLiHdzmwLGGnU+eCK0BIY8UUFdH7XgEE5HxgMXdgIrRnJwGZGFksAizc+m2Apy9LT0B0tzsTQ5S3+tNA2fP38ehUkGvsngLQmKxuutS+DMiypVqqi/x5jcNs7vICci0gcvTfUSlCVNsqx0Kezt7VWz+KRJk1TfuxyHdevWwdywRp1P3nqqEvaGR2P72Si8/8t6/J38OmxSEoESVYF6vU1dPCIiRUY5L1iwQAUvOSEYPXr0fWvGBUVWnZRsh1Krr1y5suqzjo6OztNJyrvvvqsGuEnfsgTcf//9V/1t2ih2GX0uJwCNGjVSTfG//fabCtzS5L106VKcPXtWDSArXry46h+X4yAjx80Na9T5xM7WBt+8WBsl3J2w54YdFnr3h65qF6D6s6YuGhGRwZQpU1RgkkVKJFi3bdsWdevWLfRyyHQsGZwmORwk0ZL0lUtZ8rJEdNeuXfHNN9+oZvxq1aqp0d0yilxWvRTShP3TTz+pfmvpY5cALsFcpoPJYxLUn3zySVUzl4Fvf/zxh3odc2OjK+xOg0J26dIl1U9x8eJFlCpVqsDfb+fZKLz0806kpadjQrcaeLFR2QJ/TyJ6NLJEsSQFkqx9psglQFC1WQmYUkOWkejW/rm6lIfYxBp1PmtUzgfvtJGmExt88u9RHL4cq++nDvsFuGOeIwqJiApbeHi4qu2ePHlS9RkPHDhQBbWXXnrJ1EUzOwzUBeD1FuXQqnIJ3ElNx6Dfw5C8ZASwZKh+s+4GDCKiXLG1tVV9yLIymjRNS7CWpmmpVVNWHExWAGxtbfBVj1roOG0LLtxMxLSIGnjH1h42h+cDQXWAJvdeVICIqCiQZt/sI7YpZ6xRFxAvV0fMfLkuHO1sMeOcP7ZXGKF/YM1o4OwGUxePiIgsBAN1AapZygujn9Y347xyuDZulH9Wnxbz775AdLipi0dERBaAgbqAvfxYWXSqFYTUdODZC88h1b8WcPsm8GdPDi4jIqIHYqAuYDJ5f8IzNVDOzw3h8Tq8bfsudK6+QMQh4N/hHFxGRET3xUBdCIo52WNmz3pwdrDF4nO2+KfcOMDGDjj0F7BjpqmLR0REZoyBupCEBrhjfDd9HtR393rgTN0P9A+s/gg4t8m0hSMiIrPFQF2InqlbCi82LK1au7vvq4nEKs8BujTg7z5AzP1TzBERFRRZcvPNN9803A4ODsbUqVMf2K23aNGiR37v/Hqd+5GsWLVr14alYqAuZJ90qoaqgR64mZiCV6Nehi6gFpAYpR8Jzv5qIsoDWau7Xbt2OT62efNmFQQlK1ReSVarAQMGoDCC5dWrV9G+fft8fS9rw0BdyJwd7NT8ancne2y/kIhvS3yiz7DV5jM5tTR18YjIgrz66qsqz7KsG52dJKeoX7++SkaRV35+firbVGGQNJtOTk6F8l6WioHaBMr6uOHL7vovz1e7krCqxT9A2SamLhYRWZinn35aBVVZitPYrVu38Pfff6tAHhUVpbJUlSxZUgVfyUEtWaLuJ3vT96lTp1Q6SEksIbme5eQgp2xYlSpVUu9Rrlw5lT4zJSVFPSblGzt2LA4cOKBq+bJpZc7e9C1LiUpGK0lHKVmuBgwYoP4ejeTSlqxZkjErMDBQ7TN48GDDe+U2Acinn36qkmHISYLU9FeuXGl4/M6dOxgyZIh6ffmbJS2mpOQUksdKWgfKlCmjnhsUFIRhw4ahIHEJURNpVz0QrzULwc9bzuGd+YdQJdALZXxcgSv7gP1/AO0mALZ2pi4mEd1JyPtz7JwAu4yf17RUIC0ZsLEFHFwe/LqObrl+G3t7e5UmUoLehx9+aMjlLEFa8jBLgJYgV69ePRVIPTw8sGzZMvTq1Qvly5dHw4YNcxXUnnnmGfj7+2Pnzp2IjY3N0p+tcXd3V+WQwCXBtn///uq+9957D88//zwOHz6sgqGWK9rT0/Ou10hISFCpLiXtpTS/R0ZG4rXXXlNB0/hkZP369SqIyuXp06fV60uwlffMDUmN+dVXX6m0mJLLetasWejcuTOOHDmi8nVPmzYNS5YswV9//aUCsmS4kk38888/+PrrrzFv3jyVEjMiIkKdgBTZQC0fNDlzkWTfcjDkAyBnUx999FGekoubq5HtK2PfxRjsDY/GwN/34p9+NeD827P6PmuPQKDZW6YuIhGND8r7c7rPAap1018//q9+wGjZZkDfZZn7TK2h/65nNyY2T2/Vr18/fPnll9i4caMhD7M0ez/77LMqGMr2zjvvGPYfOnQoVq1apYJQbgK1BNbjx4+r58hvsBg/fvxd/cryu2xcI5f3lGAmgVpqx5JvWk4spKn7XubOnatSQ/7yyy9wc9OfsHz77beqL37ixInqZEFIPm25387ODpUrV0bHjh2xdu3aXAdqqY3LicsLL7ygbstrS9CXVoQZM2bgwoULKmA3a9ZMxRqpUWvkMfkbWrduDQcHBxXIc3McrbbpWw7ezJkz1X/IsWPH1O1JkyZh+vTpsAYOdrb49qU68HZzxJErcfh4ZTh07b8EgpsDDV4zdfGIyAJIoGrSpImqFQqpYcpAMmn21io8kt9Zmry9vb1VwJSgKwEnN+S3VxJoaEFaSI03uz///FNlwZIgJu8hgTu372H8XrVq1TIEadG0aVNVqz9x4oThPqnJSpDWSO1aat+5ERcXhytXrqjXNSa35f2FVAj379+P0NBQ1ay9evVqw37du3fH7du3VfO+nBgsXLgQqampKLI16m3btqFLly7qbEk7S5O+lV27dsFaBHq6YOrztdFn9i78tecSynjXxJBXlkgKrsydZDS4FbQgEFmkD648XNO3pnIn/WtI07exNw8hv0hQlpqy1AalNi3N2i1btlSPSW1bmnqltijBWoKgNF1LP2x+2b59O3r27Kn6oaXpWmrxUpuW5uWC4ODgkOW21HolmOeXunXrqtzYK1asUC0KPXr0UDXo+fPnq5MWOWmQ+6WvftCgQYYWjezlKhI1ajlLlOYMSSwupB9gy5Yt9x3Kn5ycrM6YtC0+Ph7mrkUlP4zpXE1dn7z6JBbsN/ph2PwVsPxdTt0iMhXpM87rpvVPC7ku9xn3T9/vdR+CBBLJ7yxNx9JsLM3hWvegpJKUCs/LL7+saqtSE9R+U3ND8kNL/6xMo9Ls2LHjrkqVNA9LP7mMNJdm4/DwrImHHB0dVe3+Qe8lv/PSV63ZunWr+tukdpsfpJ9eWgeyp9iU2zJQzng/6fv+6aefVGuB9E3fvHlTPSZN+dIcL33ZGzZsUCcq0i9fJGvU77//vgq20rQjzRzyn/z555+rM7d7kZF5clZnaV5pHIzL0bfxw6azeG/+Qfh7OKOp+zVg7WdSpdYPLGv3BWvWRHQXaWqWoDJq1Cj1mylNtxoJmlITlGAqfbtTpkzBtWvXsgSl+5GapIzm7t27t6o5yutLQDYm7yHN3FKLbtCggRqwJk3CxqRFVGqp0qQso61loFn2aVny2/7JJ5+o95LxSdevX1ctBTL4Teufzg/vvvuueh9peZBBaNIKIeX6/fff1eNyjKQ5XQaayUmCDM6TJn0vLy81qE1iUaNGjdQIdxlDJYHbuB+7SNWoZbCDHDg5SwwLC8P//vc/NQhALu9FPqgyKlHbjh49Cksxsl1lPF0zEKnpOrzx614c15UGOmf0x+/8Hlj1IWvWRHTP5u/o6GjV9Gzcnyx9xdKUK/fLYDMJODK9KbckUEnQlX5ZGTQlo7ClwmRMRky/9dZbanS2BD45KZDpWcZkcJsszvLEE0+oKWU5TRGTwCf951JzlYD/3HPPoVWrVmqcUn6SfucRI0bg7bffVt0BMhpdRnnLCYeQkwgZDyWtA1KO8+fPY/ny5epYSLCWWrb0acscdWkC//fff9U0sYJio5NJYWZK+gKkVi1z5DTjxo1TZzAyCjE3ZCEAeR1pupGzOHOXlJKGV2btwq5zNxHo6YwFg5og8PSf+kxboslQ4CkujkKUn2SksdT2QkJC1LxZooL+XOUlNpl1jToxMVGdwRiTJvD8HDRgjiuX/dirHsr7ueFqbBL6zt6N+Go9gY5T9Dtsmw6sHcuaNRFREWHWgVo666WJRfo7pOlBml+k76Bbt4z5iVbKy9URc/o2hJ+7E45HxGPgb2G4U6cv0GGyfoctXwPrxjFYExEVAWYdqGW+tPRRyPB3GQ0oE+hff/11NSfQ2pX2dsXsPg3g6miHLadv4P0FB6GTudXtJup32DwZ2KBf0o6IiKyXWY/6lg59mfv3oHRr1qp6SU/M6FkXr/1vDxaEXUYpLxeMaPOGPjXmqg+AjRMBGzvg8ZGmLioRERXFGjUBT4SWwOddq6vr09adxrxdF4DGg/UDysSG8cCmjCZxIiKyOgzUFuCFhmUw9MkK6vqHiw5j/YlIoOkwoPUY/Q7rPgOu5j3nLBFlZc0DVclyP09m3fRNmUY8VQmXY26rJvDBv4fhr9cbo7ok7dClA66+QGDec84SUeaqWTLDRNaAljm+ctsaEv+QacisZ1miVRZskc+VfJ4eBQO1hZAfjS+eqYnIuGQ1uKzvnN1YMLAJSjd/O+uOqcmAPZOwE+WF/JjKXFdZJlOCNVF+kAVcJLtW9mnGecVAbUEc7W3x3ct10eP77WralgTrf95oAk/XjIXgE24Av3QB6vQCHnvD1MUlsihS65EfVcmE9KA1qYkeRNb8kLSe+dEyw0BtYTycHTC7bwN0m7ENpyNvof+ve/Drqw3hZG8HHP4HuHZYP8+69ouA892J2Yno3uRHVTIgFVQWJKKHwcFkFpoac06/BnB3sldLjb791wGkp+uAhgOAVp8AfZYxSBMRWQkGagtVOcAD3/eqBwc7Gyw9eBUTVx7Xr//dfATgqx8hrsRfM2UxiYjoETFQW7CmFXwx8Vn9aG9Jj/nL9vNZdzj1H/BNLWDfb6YpIBERPTIGagv3TN1SePupSur6mCVHsOaoUQ367Hog9TaweAjwS1dg7/+ARH3icyIisgwM1FZgyJMV8EKD0pBu6qF/hGHfhWj9A23GAY8Nkll9+qD97zBgckXgt+eA/XOBpFhTF52IiB6AgdpKRqqO61odj4f6ISklXa0NHh6VoO+zbjcBGBoGPDka8K8BpKcCp9cAiwYCX1YA5r4AHPwLSI439Z9BREQ5sNHJEipWLC/JuS1dQnIqnv9xOw5fjkOIrxv+GdgE3m7ZVsS5fhI4shA4sgC4fjzzfjsnoOJTwNNfA8VKFHrZiYiKkkt5iE2sUVsRNyd7zOrTACW9XHDuRgJe+99uJKVkW7jBr5I+29bgncDA7UCL9wCfCkBaMhC+FXApnrlv5DEg5Xah/x1ERJSJgdrKlHB3xv/6NYCniwPCLsRg+Lx9SJPO65z4VwWe/BAYsgd4fTPQaRpgl7HQgzS0/N5D3zx+aW+h/g1ERJSJgdoKVSjhjp9eqQ9HO1usOnINny09qhaJvyfpy5akHlU7Z94Xf1U/CE2eV6JK5v3HlwOn1gBpKQX7RxARkcIlRK1UwxBvfNWjFob+sQ9ztp1Xg8tGdaiCSv7uuXsBjyBg+EEg+hzg6Kq/T4L2f2OAGyf0TeRVOgGlHwNs7QFbO/1mk/3SFgiokdnvLdPDbp4DnD0A34qZ7yf3yYmBep69vmYvWcEecTF7IiJLx0BtxTrVCkLUrWSMW3YM609cx8aT19G9XmmMaFMJ/h7OD34BCZI+5TNvp90BQloAt28CCdeBsF/024N0nwNU66a/fnYDML8vENwc6LM0c5+fntS/rjEnDyCwVsZWGwiqDXiXZ/AmoiKFgdrK9WkaghaV/DBp5QmsPBKBP/dcxJIDV9C/eQgGtCyPYk55+AhI+syOk4H2E4HzW4Cji4DocECXBqSn6XNjq8s0o8t0wNnL6DWcAc8yd48sdyymPxGQ6WPyXLlMjgPOb9ZvxvsF1NQHbckSJv3sRERWjNOzipA9529i/PJjapCZ8C3miOGtK6nFUhzszKyWKn3g108AV/cDV/brLyMO61da07y8AKjQSn/93Gbg+FKgQmv9NDMiotySMJiaBNxJAO7cyrjUridmXnf1Aap1RWHHJtaoi5D6wd5qbvXKwxEqicf5qESMXnQYs7eew/vtKuOpqv75kjs1X0gfdUB1/VbnZf19aanAjZOZwTuoTub+p/8Ddn6v/7JpgTolCVjzsb7pXGrgvqGAHT/yRFYVYJPjgcQo/fgXdRkFJMUAHiUzB8jKftLllnwLeOZHwNVbf//aT4FdP+mDsLQIPkiZxvkWqPOCv1pFjATi9jUC0bqqP+buvIBv1p7C2esJGPDrXjQM9saoDpVRp4zRXGpzIkFWmrplq/1S1sfKP5HZh66JPArs+iFrs7t/df0odhcvfR+4k3u2LaNfXJumRkSFR7rKZMaJBFv5rmrjUY4s1He3aYHYOCjL9z4nFZ7KDNRSATm5GkhJ0C+drAVq6WaTLjZjDq6Ao1vGZTH9dW0zngFTiNj0XcTFJaXgh41n8PPmc0hO1Z9RdqwZiPfahqKsjxssWtQZYM+sjKbzA8CdXC6T+v6FzHze/74JHJoPPPEB0FjWTQcQfV5fU9cCu1zKF1q7lFHyDi6Ag3zZXTK+9C5AMX/9SHgiawmq0oIln22tJe7mWSA+AkhJ1C+WJJs0G6vrRvfJdblfBpAG1dWv5yBS7wDj/PTX3zuXGVCXjgD2/N+9yyJBVZqlZX+5lHExcsLd7M3MfWTgq8xCkdkq2vdb0gBLbVoLxPI6hfQdZdM35ZqHswPebVsZLz9WFl+tPol/wi5h2cGrWH0kQt039MmKdy9DailkxHrbzzN/VORHRJrN5VLOoqXJ7K4tTh9sNXL2LQFevuAa+SE6ujjv5XnzMOBVWn993Tgg7FfgsYGZPybyukvfygjy2ll9RsCX4K9+TDJOCAwnB8UAj1KAvYX+H1mb1OSstT3jGqAEJcP6Azqgckf9mAoRcwHYOFEfYLTPrNgwUf951dY0eOAlgND2mS1OCVHAkiH6KY/P/5r1dS/vufu5Ob2ulFkCa8W2mQFVvhdflNFf/yhSP9BUve4XwME/83bMjOuK8jl28da3aMn3UQvUFdtkBGKfrAFZ27QppPdT95W773P3l5WfYO7MPlBfvnwZI0eOxIoVK5CYmIgKFSpg9uzZqF+/vqmLZlUCPV0wuXstvNosBBNWHMemk9cxe+t5zN97CYMer4C+TYPh7GDBtUFpQvOtoN/youNXwJMfZV1a1asM0GHy3cE+KU7ftKZqEVJ70LaMWoUEWo38eN+K0NdIDPfdBE4sz/vf9sZWfV++2D4D2DFT/0MtrQBC+uVWvJc1uBu3AEiXgIyyV6P0tVH3afqBetoPpbRIXNihX25WG8AntZ/NXxk9z+i52m35wZV15O2NtiqdM6f9xVzUnzy5BwKl6mddk15qNlI27XnyOvJ6hTWOQsZE3I7Wv7fM+9daU6SFRY7bY29k7vt/bYBrR3PfaiM8S2UGavk8SN5496CsgfrUan1AzQvPjJNBIYMv5TNll+1E7kqY/rXzwsdo3QM5kdTIZ1wL1LL+gnxGjFuUZF91aXRdOwmVoOwdkvV93jt79/9xaDv9VkSZdaCOjo5G06ZN8cQTT6hA7efnh1OnTqF4cTPtQ7UCVQI98Eu/hth86jrGLz+OY1fj1MCzX7efxzttQ9G1dknY2prJgLPCoM7cM4KVRn6MGvbP2+tk72FqORKo1wdwy2jmE+4BQKdvcg7ycl0CrjTTyUmBdin3SeDV3LoGxF7U36+RgTX7f0eevbY2828/uxFYMxqo+UJmoJYAvfGLvL+uX+XMQC39joveAMo/CfRamHVefY5BzyYzaMuPudqktcNGP22wxnOZ5ZUMcbLYzktGNbzZHYH4K5nPkUvj15BLOdbagCQhJ2Xa/3fsJWDdZ/pgZByo1cjgjPLKoj1ZanwZ1+VkTwXMjHKXaZL5fAnQkuFOTp6MNXodiO+SEbhs7r5U72d8H/R/s0Zq6PKZMm4RUq/7hr4J+L6vlXEpJ0cSYGVwlkbue+d0RjePUdBuPUa/PQpzGdBqRsw6UE+cOFG14UsNWhMSku3siwpE84p+WDrUF4v2Xcbk1SdwJTYJI/46oPqyP+hQBc0q+pq6iJYl+4+PBGXZjMkPugTvRyH5x6t0yXpyIbWXVh8bBXq51FoCbukTstg6ZKwuZ5+50pxxC4BvJf2iNSXrZd4nz6n/qtFzbI2u2+sDltSqpdVAmoTlfeTSuMYn5SzdCPDLNkhHO/mQ52QZLJQxjca4JUIjr62RE5y4y/pxAcZiwvUnMnkhx8y4NUVmIchaAMae/TljNT1vwMkz74vySBNsi3fuvr9mDzwSOY45faZk8OWjKmZ0kklFdzBZ1apV0bZtW9XpvnHjRpQsWRKDBg1C//65r81wMNmjkwxcs7aew8z1ZxCfnKrua1nJD++3r6xq4EQFSsYXaEHeEPDvZPSjputbK+TSIzCzi0L6UKVvV2p7fqGZr3U5LCOgZ/TBqik52a5Lc7tWG5YaKaf0UQHIS2wy60Dt7Kxf5nLEiBHo3r07du/ejeHDh+P7779H7969c3xOcnKy2oz7uCXgM1A/upsJdzBt7Sn8tiMcqek6VUlsVdkftUt7qoAtW6Cns/nMxSYiMlNWE6gdHR3VoLFt27YZ7hs2bJgK2Nu3b8/xOWPGjMHYsWPvup+BOv+cv5GAL1edwLJDkmErKy9XB1QJ0AftKoHu6rKifzE42VvwQDQionxmNdOzAgMDVW3YWJUqVfDPP//c8zmjRo1SNfDsNWrKP8G+bpjRsy4GXo7FtjM3cOxqvBp0djryFmISU7D9bJTaNPa2NqhQoliW4C2bb7GMkaJERGSZgVpGfJ84cSLLfSdPnkTZsmXv+RwnJye1aeLisq06Q/mmeklPtWmSU9Nw6totFbS14H30ahxib6fgeES82hbuy3y+n7sTqmYEbQngcj3E1w325rbuOBGRpQVqqapLP6RWXd+1axfmzp2raq4DBgzIt8K99dZbaNKkCcaPH48ePXqo9/nxxx/VRuZHmrezB2/pWbkam5QRvPUBXIL3+agEXI9PxsZ4ffrNzNewRWiAPmi3qeaPlpVKwK4oTQcjIsqPPurmzZurgNyrVy9EREQgNDQU1apVU3Ochw4dio8//hj5ZenSpao5W15bpmZJszZHfVu+hORUnLiWUeu+og/iUuNOvJOWZb+SXi54qVEZPN+gNJvKichqFPhgMllwZMeOHSpAT5s2DX/++Se2bt2K1atX44033sDZs7LknXlgoLYc6ek6XLiZqIL27vPRWLDvkurzFg52NmhfPRC9GpdF/bLFObKciCxagQ8mS0lJMfQD//fff+jcWZ+hpHLlyrh69e6RwES5ISueyUA12STD13vtQrH04FU1HWz/xRgsOXBFbaH+7ni5cVl0q1MSxZzMepgFEdEje6hRO9LMLXOZN2/ejDVr1qBdO/0arFeuXIGPj8+jl4pI5tE72OG5eqWwaHBT/DukGZ6vXxrODraqyVzyaDf6/D98tOgQjkdwwCARWa+HavresGEDunXrpkZUy8Ijs2bNUvd/8MEHOH78OBYsWABzwaZv6yIjyP/Zewm/7QxXebQ1DYKLq2xf7aoHcM42EZm9QlnwJC0tTQVq4wQZ58+fh6urK0qUKAFzwUBtneRju/1MlArYq45cQ1q6/mPs4+aoBp692LAMSnvnIvUdEZE19lHfvn1b/VBqQTo8PBwLFy5Ui5HI2txEBU0GkzWp4Ku2a3FJmLfrIubuCse1uGR8t+EMZm48gydDS6hadotKfpziRUQW66Fq1G3atMEzzzyjRnjHxMSoQWQODg64ceMGpkyZgoEDB8JcsEZddKSmpeO/Y5Fq8NmW0zcM95cq7oKejcqiR/1S8OEULyKysNj0UIPJwsLC1FxqMX/+fPj7+6ta9S+//KKmaxGZgqxoJn3Uv73WCOvebolXm4XAw9kel6Jvq5zajSesw5vz9mFveLSpi0pElGsPFagTExPh7q5PcC5zp6V2bWtri8cee0wFbCJTK+dXDKOfroqdH7TGpOdqomYpT9xJS8ei/Vfw7MxtGDw3TDWZExFZZaCuUKECFi1apKrsq1atUk3hIjIyEh4ezE9M5sPF0Q496pfGkiHNsGRIUzXdS7qrlx28ilZfbcTsrecMA9GIiKwmUMsSoe+88w6Cg4PRsGFDNG7c2FC7rlOnTn6XkShf1Czlhcnda6mgXbu0F24lp2Lsv0fRZcYWHLgYY+riERHl7/QsWeNbViGrVauWavYWkjRDatQyuMxccDAZ3Wu50j92X8DEFccRl5QKWZH05UZl8U7bUHi6OJi6eERk5S4Vxjxq4zcT5hoEGajpfiSD14Tlx7Bg32V1WxJ/jH66CjrXCuJ64kRkuaO+09PT8emnn8LT01PlhpbNy8sLn332mXqMyFJITuwpz9fG3NcaoZyfG27cSsbwefvx8v/txNnrt0xdPCKihwvUH374Ib799lt88cUX2Ldvn9okZ/T06dMxevTo/C8lUQGThVNWDG+Od9pUUjmxt56OQrupmzFlzUkkpWRNvUlEVJgequk7KChIJeXQsmZpFi9ejEGDBuHyZX0zojlg0zflVXhUAj5efAQbT15Xt8v6uOLTLtXRspKfqYtGRFaiwJu+b968meOAMblPHiOyZGV93DCnbwN817Mu/D2cEB6ViN6zdnHuNRGZxEMFahnpLU3f2cl9NWvWzI9yEZmUDCTrUCMQ/41oiX5NQzj3mogsq+l748aN6NixI8qUKWOYQ719+3ZVhV++fLlheVFzwKZvyg+HL8fiw0WHDfOtq5f0wOdda6BWaS9TF42ILFCBN323bNkSJ0+eVDmpJSmHbLKM6JEjR/Drr78+bLmJzFb1kp5YMLAJxnWtDndnexy+HIeu323F6EWHVY5sIqKC8sjzqI0dOHAAdevWVbmqzQVr1FQQc6/HLz+GhZx7TUTmWqMmKupzr7/W5l77Zp17ffBSjFr1jIgov9jn2ysRFcW51282x48bz2L6+tNq7nXnb7fCt5gjmlXwRYtKfuqyhIezqYtKRBaMgZroETjZ22Foq4roXDsIk1aewLrjkbhx645KpymbqBzgroJ284q+aBDsDWcHO1MXm4isNVDLgLH7kUFlREV17vWMnnWRnJqGsPAYbD51HZtP3cDhK7E4HhGvth83nVWrnjUM8UaLin5oXskXof7u7NcmovwL1LK294Mef+WVV/LykkRWV8NuXN5Hbe+1A6JuJWPrmShsPqkP3BFxSepSNizX93dLTVsCd9MKvuo2EVGBjfouaLK2+KhRozB8+HBMnTo1V8/hqG8yF/JVOx15C5tUoL6OHWejkJSSNYlN1UAPVdOWwF0/uLgK/ERkffISmyymj3r37t344YcfuPIZWSxp4q7o7662V5uFqGQfYeHRhsB95Eocjl7Vbz9sPAtnB1s8Vs4HzaWZvKIvKpYoxmZyoiLIIgL1rVu30LNnT/z0008YN26cqYtDlC9kUJmMHJft/faV1TSvradvqGQg0jQu87U3nLiuNlHC3Uk1j+s3HwR6upj6TyCiQmARgXrw4MFqydLWrVs/MFAnJyerTRMfH18IJSR6dLJwSpfaJdUmzeQnrsVj88kb2HTqOnadu4nI+GS1yIq20Irkz26WEbil5u3p4mDqP4GIimKgnjdvHsLCwlTTd25MmDABY8eOLfByERUkaeKuHOChtv4tyhmaybecvqEGpx26FIOz1xPU9sv2cJU0pEYpLzQt76OCd92yxTkNjMhKmPVgMulkr1+/PtasWWPom3788cdRu3btew4my16jltzYVatW5WAysiqxiSnYfjYK287cUMFbArYxmQYmc7alti2Bu2qQB+wkmhORxQ0mM+tAvWjRIpX4w84us2Yg64hLbcPW1lYFZOPHcsJR31QUXI29rVZGkz5u2aSZ3Jg0izcp76P6wyVwB/u4cmAakQlZTaCW/uXw8PAs9/Xt2xeVK1fGyJEjUb169Qe+BgM1FdVpYKqZ/HSUmgZ2Kzk1yz4lvVxU4G5WUR+4fYpx/jZRYbKa6Vnu7u53BWM3Nzf4+PjkKkgTFfVpYH2bhiA1LR0HLsVi22l9M3nYhWhcjrmNv/deUps0k49qXxmvNA6GLZvHicyOWQdqInp09na2qFe2uNpkXfLEO6nYfT5aPxXsxHU1unzMv0fx37FIfNm9Jqd9EZkZs276zg9s+ia6N/n6/7ojXOXXllXSPJzt8VnX6mqKGBEVHOajJqJcN5NLk/eyYc1Rq5Qn4pJSVW7tIXPDEJN4x9TFIyIGaiIS5f2KYf7AJnizdUU1jWvpwatoO3WTWiWNiEyLgZqIFAc7W7zZuhIWDGyiVj27FpeM3rN24ePFh3H7Tpqpi0dUZDFQE1EWtUp7YdnQ5ujduKy6LSufdZy2GfsvMt88kSkwUBPRXVwc7TC2S3X8+mpDBHg44+yNBDw7cxu+XnMSKWlZU3MSUcFioCaie5IUm6vebIHOtYKQlq7DN2tPqYAtC6oQUeFgoCai+/J0dcC0F+uoTaZvHbwUq5rC52w9h/R0q57dSWQWGKiJKFekVr36rZZoXtEXyanpapGUV2btUuuME1HBYaAmolwL8HTGL/0a4tMu1eDsYKuWJG379SYs3q/PkU1E+Y+BmojyhIukEBUuBmoieihcJIWocDBQE9FD4yIpRAWPgZqICmyRlP+OXsOdVM67JnoUTHNJRPm6SErrqv549++DapGU137Zo6Z0PVU1AB1rBqBZBT842rN+QJQXDNREVCCLpMjiKEsPXkFkfDL+CbukNncVtP3RoXogmlfyhZO9namLS2T2mI+aiAqMrGa2Nzwayw9dVZsEbY27k72qfXeoEajmZjs7MGhT0XEpD7GJgZqICoWsYrb3QjSWHbyKFYevqoFnmmIStKuUUEG7RSU/Bm2yepcYqDMxUBOZZ9AOk6B96CpWHIpARFxSlqDdKiNot2TQJivFQG2EgZrI/IP2votS045QNe2rsZlB283RDq2q6JvHHw9l0CbrwUBthIGayNKCdozqz15x6CquZAvaT1bxR8caAXg8tASDNlk0BmojDNRElhu091+KwXLVpx2ByzGZyT9cHOxQP7g4Gpf3QeNyPqhR0hP2dpz2RdYZmzg9i4jMkq2tDeqWKa62DztWwYFLsaqmLYPRJGhvPnVDbVq/dgND4PZF1SAPtawpkTVgoCYii0gEUru0l9pGta+ME9fisf1MlNp2nruJ2NspWH/iutqEzNduFOKNx8r5qOBdJcBDBX4iS8RATUQWF7QrB3iorW/TEDVX+9jVOOw4qw/cu87dRHxSKv47Fqk24eXqoAK3NJM3Lu+LSv7F1OsQWQKzDtQTJkzAggULcPz4cbi4uKBJkyaYOHEiQkNDTV00IjIT0sRdvaSn2l5rXg6paek4ciUO2zMC9+7zNxGTmIJVR66pTfi4Oara9mMZfdzl/dwYuMlsmfVgsnbt2uGFF15AgwYNkJqaig8++ACHDx/G0aNH4ebmlqvX4GAyoqItJS0dhy7HqqAttW4J3EkpWROF+Lk7qYAtwVv6ukN83Tg4jQqU1Y76vn79OkqUKIGNGzeiRYsWuXoOAzURGZNsXgcuxRj6uGW1tOwZvhztbFG+RDGE+hdDqGpmd0elAHcEeTqz5k35wmpHfcfGxqpLb29vUxeFiCyUZO9qEOyttmGtKiIpJQ37LsSopvIdZ6Jw+EosEu+kqX5v2YArhufKILVQf3eEBmRs/u6qr9zT1cGkfxNZN4upUaenp6Nz586IiYnBli1b7rlfcnKy2jSXL19G1apVWaMmolzP35bpX8cj4nHyWry6PBERh7PXE5CanvPPpb+Hk6HmrQXyCiWKcVEWKlo16sGDB6v+6fsFaW0A2tixYwutXERkXWQaV2lvV7VJSk6NNI+fvXELJ1Tg1m8SxCWoS4KRa3HXsenk9czXsQGCfd0MgVuCeNMKvnB3Zu2brLBGPWTIECxevBibNm1CSEjIffdljZqIClN8UgpOXtMCeJya4y3XoxNT7tpXFmbpXr8U+jYJQRkfV5OUl8yD1dSo5Rxi6NChWLhwITZs2PDAIC2cnJzUpomLkz4mIqKCITXkemWLq834t+t6fLIhaEvNW/Jyn7uRgNlbz+N/286r2vqrzcqpUeYcoEYWG6iluXvu3LmqNu3u7o6IiAh1v6enp5pXTURkjiTwlvBwVlvzin6G4L3x5HXM2npeNZFr87plnfJXm4WoDGEy0I3Iopq+73WWOXv2bPTp0ydXr8HpWURkbmSQ2uyt57Ag7DKSM6aGyYC0VxoH46WGZVDczdHURaQCZrXzqB8GAzURmauoW8mYu/MCftkRrprKhbODLZ6pWwr9moaokeNknRiojTBQE5G5S05Nw9IDV/F/W87hqJq7rfd4qJ9qFm9WwZf92FbGagaTEREVBU72dni2Xik8U7ekygYmAfu/Y9ew4cR1tckUr37NgtGldknOzS6CWKMmIjJD528kYM628/hrz0W1UpqWTKTnY2XR67Gyan1yslxs+jbCQE1Elkxybf+5+wL+ty1cLa6irUXeqVaQahavGuRh6iLSQ2CgNsJATUTWQNJ3rjwSoZrFZW1yjWT96tM0GM0r+sLVkb2ZloJ91EREVkbSbj5dM0htYReiMWvLOaw4HKHPu302Cg52NqhTpjialvdFs4o+qFnKCw5M1WkVGKiJiCxM3TLFUfel4qop/Jdt57H04FV1fde5m2r7+j/AzdEOjcr5qPXFm1bwUQPSOHLcMrHpm4jIwsnP+IWbidhy+ga2nY7CtjM37lpr3LeYE5qUl8CtD96linOtcVNi0zcRUREiNeWyPm5q69morErVKfOxJWBvPR2latk3biVjyYErahNlfVz1te3yvmhc3gfeXA3NbDFQExFZYarO6iU91TagRXmVonPfhWhsPX0DW89EYf/FGIRHJSI86oJaGU1axKsGeqjALbXuhiHeHJhmRtj0TURUBFNzSi1batsSvCXLlzFtYJqsiCZBWxKHuDkxcOcnNn0TEdF9U3O2quKvNhEZn4TtZ/RBW4K38cA0YWsDVCzhjlqlPVGrtBdqlfJCaIA7R5UXEgZqIqIiroS7s1qeVDZpZJVm8a1n9APTpMn8SmySPrf2tXj8teeSeo6Tva1qWpegLQG8dmkvlPF25cjyAsBATUREBhJog33d1CYD00RkXBIOXIrFgYsxOHApRvVxxyelYm94tNo0Xq4OGYHbC7Wl9l3KCz7FuNTpo2KgJiKi+yrh4YynqsqmbyqXUeXnoxJU0D5wMVYF7qNX4hCTmIKNJ6+rTVOquIs+cGcE8OolPThQLY94tIiIKM+jysv5FVNbtzr6gVAysvx4RJyqde+/GKuC+OnIW7gUfVttyw5e1T/XBqjk765q2+X89FPKZKqYbAzgOeNRISKiR+Zob6uWLZWtV2P9fXFJKTh8KRb7Vc1bX/uOiEvC8Yh4tWUni7IE+7iijARub30AL+PjimAfNxR3dSiy/d8M1EREVCA8nB3QROZmV/A13BcRK/3dMTh8ORbnoxJxISoB4TcTVbO5LMoi2x6jfm+Nu5O9PoBL8PZ2ywzoPm4I9HBWtXxrxUBNRESFJsDTGQGeAWhbLSDL/bGJKQi/maBGnMtyqOESwNWiLImqFh6fnIojV+LUlp2k/Szl7aJq3jLyXLYgLxfVPy6Xll4bZ6AmIiKT83R1QE1XfdN5dkkpabiogneiqn1rQVwC+qXoRNxJS8fZ6wlqy4mzg60K2CUzNrlufFtOHqTp3lwxUBMRkVlzdrBDRX93tWWXlq7DlZjbGUE8AReiEnExOhGXY5LU/dfjk5GUcv9ALpVtv2JOKJlRA1fB3NNZf724/rani+lq5QzURERksexsbVDa21VtzZDZF65JTk1T/eKy2trl6Nu4khHAr8Tqb8v9yanpiIxPVtu+CzE5vo+ro50K3NWDPDD1hTooTAzURERktZzs7QyZxXIiK7HdTLijArgK5hLEjTapmcsAt8Q7aWq6mSnWPGegJiKiIsvGxkatniZbjVKeOe4jfeRXY/U1cVM0fjNQExERPaCPPMTXTW2mYL7D3IzMmDEDwcHBcHZ2RqNGjbBr1y5TF4mIiKhQmH2g/vPPPzFixAh88sknCAsLQ61atdC2bVtERkaaumhEREQFzuwD9ZQpU9C/f3/07dsXVatWxffffw9XV1fMmjXL1EUjIiIq2oH6zp072Lt3L1q3bm24z9bWVt3evn17js9JTk5GXFycYYuPv3s9WSIiIkth1oH6xo0bSEtLg7+/PrWaRm5HRETk+JwJEybA09PTsEktnIiIyFJZ3ajvUaNGqT5tzcWLF1G9enVcvapPsUZERGRqWkxKT0+37EDt6+sLOzs7XLt2Lcv9cjsgIOuC7honJye1aRITE9Vlw4YNC7i0REREeSPxrEyZMpYbqB0dHVGvXj2sXbsWXbt2NZx9yO0hQ4bk6jXq1KmjpnNJc7n0bz8K6e+WpvSjR4/C3f3uNWfpbjxmecdjlnc8ZnnHY2baYyaxTIK0xKgHsdHJ+mlmPj2rd+/e+OGHH1SteOrUqfjrr79w/Pjxu/quC5oMTpN+79jYWHh4eBTqe1sqHrO84zHLOx6zvOMxs5xjZtY1avH888/j+vXr+Pjjj9UAstq1a2PlypWFHqSJiIhMwewDtZBm7tw2dRMREVkTs56eZW5kkJqskGY8WI3uj8cs73jM8o7HLO94zCznmJl9HzUREVFRxho1ERGRGWOgJiIiMmMM1ERERGaMgToPmBc792TN9QYNGqhFAUqUKKEWrDlx4oSpi2UxvvjiC9jY2ODNN980dVHM2uXLl/Hyyy/Dx8cHLi4uqFGjBvbs2WPqYpktyZ0wevRohISEqONVvnx5fPbZZ+BQpaw2bdqETp06ISgoSH0PFy1alOVxOV4yZTgwMFAdR0kUderUKRQUBupcYl7svNm4cSMGDx6MHTt2YM2aNUhJSUGbNm2QkJBg6qKZvd27d6sFfmrWrGnqopi16OhoNG3aFA4ODlixYoVaLeqrr75C8eLFTV00szVx4kTMnDkT3377LY4dO6ZuT5o0CdOnTzd10cxKQkKC+o2XyllO5JhNmzZNpV3euXMn3NzcVDxISkoqmALJqG96sIYNG+oGDx5suJ2WlqYLCgrSTZgwwaTlshSRkZFyyq7buHGjqYti1uLj43UVK1bUrVmzRteyZUvd8OHDTV0kszVy5Ehds2bNTF0Mi9KxY0ddv379stz3zDPP6Hr27GmyMpk7ALqFCxcabqenp+sCAgJ0X375peG+mJgYnZOTk+6PP/4okDKwRl1AebEpK1lyT3h7e5u6KGZNWiE6duyY5bNGOVuyZAnq16+P7t27q+4VWTP5p59+MnWxzFqTJk1UroSTJ0+q2wcOHMCWLVvQvn17UxfNYpw7d06tkmn8HZVlRaU7tKDigUWsTGbOebFlzXF68OLz0tcqzZSScpRyNm/ePNWtIk3f9GBnz55VzbjSJfXBBx+o4zZs2DCVzEfyA9Dd3n//fbVedeXKlVVmQvld+/zzz9GzZ09TF81iREREqMuc4oH2WH5joKZCqSUePnxYnblTziRv+vDhw1V/vgxWpNydAEqNevz48eq21Kjlcyb9hgzUOZOERr///jvmzp2LatWqYf/+/eokWgZN8ZiZLzZ9F1BebNKTNdqXLl2K9evXo1SpUqYujtmSrhUZmFi3bl3Y29urTQbkyYAVuS41H8pKRtxKykFjVapUwYULF0xWJnP37rvvqlr1Cy+8oEbI9+rVC2+99ZaapUG5o/3mF2Y8YKDOY15sjZYXu3HjxiYtm7mSMRgSpBcuXIh169ap6SB0b61atcKhQ4dUDUfbpLYoTZJyXU4UKSvpSsk+5U/6XsuWLWuyMpm7xMRENb7GmHy25PeMckd+yyQgG8cD6U6Q0d8FFQ/Y9J1L0g8mTUPy46nlxZYh/H379jV10cy2uVua1xYvXqzmUmt9NzLoQuYdUlZyjLL338uUD5kfzH79nElNUAZHSdN3jx491LoGP/74o9ooZzI3WPqky5Qpo5q+9+3bhylTpqBfv36mLppZuXXrFk6fPp1lAJmcMMtgWDl20l0wbtw4VKxYUQVumZsu3QeyXkSBKJCx5FZq+vTpujJlyugcHR3VdK0dO3aYukhmSz5aOW2zZ882ddEsBqdnPdi///6rq169upoaU7lyZd2PP/5o6iKZtbi4OPWZkt8xZ2dnXbly5XQffvihLjk52dRFMyvr16/P8ferd+/ehilao0eP1vn7+6vPXqtWrXQnTpwosPIwexYREZEZYx81ERGRGWOgJiIiMmMM1ERERGaMgZqIiMiMMVATERGZMQZqIiIiM8ZATUREZMYYqImIiMwYAzUR5TsbGxssWrTI1MUgsgoM1ERWpk+fPipQZt/atWtn6qIR0UNgUg4iKyRBefbs2Vnuc3JyMll5iOjhsUZNZIUkKEsqPuOtePHi6jGpXc+cORPt27dXmczKlSuH+fPnZ3m+pNx88skn1eOSwWvAgAEqo5CxWbNmqQxM8l6SG1rSmhq7ceMGunXrBldXV5VlaMmSJYbHoqOjVQpPPz8/9R7yePYTCyLSY6AmKoIkLd+zzz6LAwcOqID5wgsv4NixY+oxSd/atm1bFdh3796Nv//+G//991+WQCyBXlKZSgCXoC5BuEKFClneY+zYsSr95MGDB9GhQwf1Pjdv3jS8/9GjR7FixQr1vvJ6vr6+hXwUiCxEgeXlIiKTkFR8dnZ2Ojc3tyzb559/rh6Xr/0bb7yR5TmNGjXSDRw4UF2XVJHFixfX3bp1y/D4smXLdLa2trqIiAh1OygoSKVHvBd5j48++shwW15L7luxYoW63alTJ13fvn3z+S8nsk7soyayQk888YSqpRqTpPeaxo0bZ3lMbu/fv19dlxpurVq14ObmZni8adOmSE9Px4kTJ1TT+ZUrV9CqVav7lqFmzZqG6/JaHh4eiIyMVLcHDhyoavRhYWFo06YNunbtiiZNmjziX01knRioiayQBMbsTdH5RfqUc8PBwSHLbQnwEuyF9I+Hh4dj+fLlWLNmjQr60pQ+efLkAikzkSVjHzVREbRjx467blepUkVdl0vpu5a+as3WrVtha2uL0NBQuLu7Izg4GGvXrn2kMshAst69e+O3337D1KlT8eOPPz7S6xFZK9aoiaxQcnIyIiIistxnb29vGLAlA8Tq16+PZs2a4ffff8euXbvwf//3f+oxGfT1ySefqCA6ZswYXL9+HUOHDkWvXr3g7++v9pH733jjDZQoUULVjuPj41Uwl/1y4+OPP0a9evXUqHEp69KlSw0nCkSUFQM1kRVauXKlmjJlTGrDx48fN4zInjdvHgYNGqT2++OPP1C1alX1mEynWrVqFYYPH44GDRqo29KfPGXKFMNrSRBPSkrC119/jXfeeUedADz33HO5Lp+joyNGjRqF8+fPq6b05s2bq/IQ0d1sZERZDvcTkZWSvuKFCxeqAVxEZP7YR01ERGTGGKiJiIjMGPuoiYoY9nYRWRbWqImIiMwYAzUREZEZY6AmIiIyYwzUREREZoyBmoiIyIwxUBMREZkxBmoiIiIzxkBNRERkxhioiYiIYL7+H45qPjbiU/VSAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True)) \n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567f467",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(inference_device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(inference_device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"출력 텍스트:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e99215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chap_5 import generate\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(inference_device),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"출력 텍스트:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40842f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"holmes_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
